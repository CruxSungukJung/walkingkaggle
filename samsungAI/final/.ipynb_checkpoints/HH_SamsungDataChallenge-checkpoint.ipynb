{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"./dataset/train_data.csv\",encoding = 'CP949')\n",
    "test_data = pd.read_csv(\"./dataset/test_data.csv\",encoding = 'CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testdata에 있는 columns 만 쓰기 위함\n",
    "columns = test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(\"./dataset/raw_train.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder를 위해 합침\n",
    "merge = pd.concat([train_data, test_data])\n",
    "\n",
    "merge.to_csv(\"./dataset/raw_merge.csv\", encoding=\"CP949\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리컬데이터은 레이블인코더로 바뀌고 테스트에 없는건 0으로 바뀜,\n",
    "# 이를 위해 float데이터도 모두 1을 더해주고 없는 값만 0으로 해줌\n",
    "# merge후 저장\n",
    "\n",
    "objcol = [] # 레이블 갯수 확인용\n",
    "\n",
    "for col in columns:\n",
    "    if merge[col][0].dtype == object:\n",
    "        le=LabelEncoder()\n",
    "        y=merge[col].tolist()\n",
    "        le.fit(y)\n",
    "        merge[col]=le.transform(y)\n",
    "        objcol.append(col)\n",
    "    else:\n",
    "        # float 데이터 처리\n",
    "        y = merge[col].tolist()\n",
    "        for i in range(len(y)):\n",
    "            \n",
    "            # nan이면 0으로\n",
    "            if np.isnan(y[i]):\n",
    "                y[i] = 0\n",
    "            # 데이터 있으면 +1\n",
    "            else:\n",
    "                y[i] += 1\n",
    "        merge[col] = y\n",
    "        \n",
    "merge.to_csv(\"./dataset/labeled_merge.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "주야 : 2\n",
      "요일 : 7\n",
      "발생지시도 : 17\n",
      "발생지시군구 : 208\n",
      "사고유형_대분류 : 4\n",
      "사고유형_중분류 : 19\n",
      "법규위반 : 20\n",
      "도로형태_대분류 : 9\n",
      "도로형태 : 16\n",
      "당사자종별_1당_대분류 : 12\n",
      "당사자종별_2당_대분류 : 14\n"
     ]
    }
   ],
   "source": [
    "## 오브젝트 레이블 갯수확인 (null값 빼고)- 모델 설계할 때 필요.\n",
    "for i, obj in enumerate(objcol) :\n",
    "    print(obj, \":\", max(merge[obj].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다시 분리 후 저장\n",
    "\n",
    "train_data = merge[:train_data.shape[0]]\n",
    "test_data = merge[train_data.shape[0]:]\n",
    "\n",
    "train_data.to_csv(\"./dataset/labeled_train.csv\", encoding=\"CP949\", index = False)\n",
    "test_data.to_csv(\"./dataset/labeled_test.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모두 1인 valid mask를 포함한 test data의 빈칸 뚤린 종류의 갯수: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "## feature 갯수 지정\n",
    "inputDim = test_data.shape[1]\n",
    "\n",
    "## 학습을 위해 test셋의 빈칸 종류들을 넣기위함\n",
    "## 전부 다 차있을 때 를 학습 시키기 위해 모두 1인 값을 넣어줌\n",
    "valid_list = [[1]*inputDim]\n",
    "\n",
    "## predict를 위해 만들어줌\n",
    "test_masks = []\n",
    "\n",
    "## 비어있는 곳은 0 차있는 곳은 1로 valid mask를 만들어서 넣어줌.\n",
    "\n",
    "for i in range(test_data.shape[0]) :\n",
    "    valid_mask = test_data.ix[i].tolist()\n",
    "    \n",
    "    for i, mask in enumerate(valid_mask):\n",
    "        if not mask == 0 :\n",
    "            valid_mask[i] = 1\n",
    "    \n",
    "    test_masks.append(valid_mask)\n",
    "    \n",
    "    if not valid_mask in valid_list :\n",
    "        valid_list.append(valid_mask)\n",
    "        \n",
    "print(\"모두 1인 valid mask를 포함한 test data의 빈칸 뚤린 종류의 갯수:\", len(valid_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input = Input(shape=(inputDim,), name=\"data_input\")\n",
    "valid_input = Input(shape=(inputDim,), name=\"valid_input\")\n",
    "\n",
    "## data_input이랑 valid_input 합치기\n",
    "x = concatenate([data_input,valid_input])\n",
    "\n",
    "## AutoEncoder 랑 비슷하게 만듬\n",
    "x = Dense(64,activation='tanh')(x)\n",
    "x = Dense(128,activation='tanh')(x)\n",
    "x = Dense(64,activation='tanh')(x)\n",
    "x = Dense(32,activation='tanh')(x)\n",
    "\n",
    "## 카테고리컬 아웃풋은 레이블 갯수만큼 맞춰줌 활성화함수도 맞춰서 계산\n",
    "output_1 = Dense(2, activation = 'softmax' , name = 'output_1')(x)\n",
    "output_2 = Dense(7, activation = 'softmax', name = 'output_2')(x)\n",
    "output_3 = Dense(1, activation = 'linear' , name = 'output_3')(x)\n",
    "output_4 = Dense(1, activation = 'linear' , name = 'output_4')(x)\n",
    "output_5 = Dense(1, activation = 'linear' , name = 'output_5')(x)\n",
    "output_6 = Dense(1, activation = 'linear' , name = 'output_6')(x)\n",
    "output_7 = Dense(1, activation = 'linear' , name = 'output_7')(x)\n",
    "output_8 = Dense(17, activation = 'softmax', name = 'output_8')(x)\n",
    "output_9 = Dense(208, activation = 'softmax', name = 'output_9')(x)\n",
    "output_10 = Dense(4, activation = 'softmax', name = 'output_10')(x)\n",
    "output_11 = Dense(19, activation = 'softmax', name = 'output_11')(x)\n",
    "output_12 = Dense(20, activation = 'softmax', name = 'output_12')(x)\n",
    "output_13 = Dense(9, activation = 'softmax', name = 'output_13')(x)\n",
    "output_14 = Dense(16, activation = 'softmax', name = 'output_14')(x)\n",
    "output_15 = Dense(12, activation = 'softmax', name = 'output_15')(x)\n",
    "output_16 = Dense(14, activation = 'softmax', name = 'output_16')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model 설정\n",
    "\n",
    "model = Model(inputs=[data_input, valid_input],\n",
    "              outputs=[output_1, output_2, output_3, output_4, output_5, output_6, output_7, output_8, \n",
    "                       output_9, output_10, output_11, output_12, output_13, output_14, output_15, output_16])\n",
    "\n",
    "model.compile(optimizer='Adam',\n",
    "              loss = {'output_1': 'binary_crossentropy',\n",
    "                     'output_2' : 'categorical_crossentropy',\n",
    "                     'output_3' : 'mean_squared_error', \n",
    "                     'output_4' : 'mean_squared_error', \n",
    "                     'output_5' : 'mean_squared_error', \n",
    "                     'output_6' : 'mean_squared_error', \n",
    "                     'output_7' : 'mean_squared_error', \n",
    "                     'output_8' : 'categorical_crossentropy', \n",
    "                     'output_9' : 'categorical_crossentropy', \n",
    "                     'output_10' : 'categorical_crossentropy', \n",
    "                     'output_11' : 'categorical_crossentropy', \n",
    "                     'output_12' : 'categorical_crossentropy', \n",
    "                     'output_13' : 'categorical_crossentropy', \n",
    "                     'output_14' : 'categorical_crossentropy', \n",
    "                     'output_15' : 'categorical_crossentropy',\n",
    "                     'output_16' : 'categorical_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_val(train_data, val_size):\n",
    "\n",
    "    val_idx = np.random.choice(range(len(train_data)), val_size, replace=False)\n",
    "    val_data = train_data[val_idx]\n",
    "    sub_train_data = np.delete(train_data, val_idx, 0)\n",
    "\n",
    "    return sub_train_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training 위해서 array로 바꿔줌\n",
    "\n",
    "train_data = train_data[train_data.columns[:]]\n",
    "train_data = np.array(train_data)\n",
    "test_data = test_data[test_data.columns[:]]\n",
    "test_data = np.array(test_data)\n",
    "\n",
    "valid_list = np.array(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train_data 다시 보정, 0값은 null값이므로 기존의 0 레이블을 없애줌\n",
    "\n",
    "train_data -= [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs 1/10  \n",
      "\n",
      "1/16. valid_mask: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 13s 745us/step - loss: 23.5208 - output_1_loss: 0.6846 - output_2_loss: 1.9516 - output_3_loss: 0.0920 - output_4_loss: 2.3984 - output_5_loss: 0.7774 - output_6_loss: 0.8438 - output_7_loss: 0.3453 - output_8_loss: 2.5845 - output_9_loss: 5.1421 - output_10_loss: 0.7327 - output_11_loss: 1.8898 - output_12_loss: 1.2437 - output_13_loss: 0.6010 - output_14_loss: 1.1337 - output_15_loss: 1.5672 - output_16_loss: 1.5330\n",
      "\n",
      " Validation Accuracy - Numerical: 0.4256414978951398 Categorical: 0.0740666172425083, Total: 0.18393376744645562\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 617us/step - loss: 17.8543 - output_1_loss: 0.6493 - output_2_loss: 1.8242 - output_3_loss: 0.0793 - output_4_loss: 1.2416 - output_5_loss: 0.6038 - output_6_loss: 0.7106 - output_7_loss: 0.1311 - output_8_loss: 2.2110 - output_9_loss: 4.9391 - output_10_loss: 0.2755 - output_11_loss: 1.1898 - output_12_loss: 0.9695 - output_13_loss: 0.2549 - output_14_loss: 0.7192 - output_15_loss: 1.1772 - output_16_loss: 0.8782\n",
      "\n",
      " Validation Accuracy - Numerical: 0.3700583137151255 Categorical: 0.0755871550027325, Total: 0.1676093921003553\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 622us/step - loss: 15.7229 - output_1_loss: 0.5561 - output_2_loss: 1.5580 - output_3_loss: 0.0751 - output_4_loss: 1.3100 - output_5_loss: 0.4018 - output_6_loss: 0.5570 - output_7_loss: 0.3825 - output_8_loss: 1.9244 - output_9_loss: 4.6841 - output_10_loss: 0.1376 - output_11_loss: 0.9273 - output_12_loss: 0.8028 - output_13_loss: 0.1962 - output_14_loss: 0.5846 - output_15_loss: 0.9920 - output_16_loss: 0.6335\n",
      "\n",
      " Validation Accuracy - Numerical: 0.42220630255014313 Categorical: 0.0702823413441236, Total: 0.18025857922100472\n",
      "2/16. valid_mask: [1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 630us/step - loss: 15.7016 - output_1_loss: 0.3879 - output_2_loss: 1.3329 - output_3_loss: 0.0764 - output_4_loss: 1.9572 - output_5_loss: 0.4317 - output_6_loss: 1.2167 - output_7_loss: 0.0844 - output_8_loss: 1.8098 - output_9_loss: 4.5271 - output_10_loss: 0.0922 - output_11_loss: 0.8512 - output_12_loss: 0.7462 - output_13_loss: 0.1797 - output_14_loss: 0.5458 - output_15_loss: 0.9195 - output_16_loss: 0.5430\n",
      "\n",
      " Validation Accuracy - Numerical: 0.3193303480532103 Categorical: 0.0728565195904945, Total: 0.14987959098509318\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 638us/step - loss: 15.1077 - output_1_loss: 0.1632 - output_2_loss: 1.1912 - output_3_loss: 0.0746 - output_4_loss: 2.2144 - output_5_loss: 0.3276 - output_6_loss: 1.1930 - output_7_loss: 0.3655 - output_8_loss: 1.6904 - output_9_loss: 4.3886 - output_10_loss: 0.0580 - output_11_loss: 0.7810 - output_12_loss: 0.6743 - output_13_loss: 0.1678 - output_14_loss: 0.5233 - output_15_loss: 0.8072 - output_16_loss: 0.4876\n",
      "\n",
      " Validation Accuracy - Numerical: 0.4478552498418966 Categorical: 0.0768209052573615, Total: 0.19276913794002873\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 600us/step - loss: 14.2740 - output_1_loss: 0.1022 - output_2_loss: 1.1001 - output_3_loss: 0.0735 - output_4_loss: 2.1165 - output_5_loss: 0.3434 - output_6_loss: 1.1549 - output_7_loss: 0.3118 - output_8_loss: 1.5481 - output_9_loss: 4.2478 - output_10_loss: 0.0407 - output_11_loss: 0.7427 - output_12_loss: 0.6304 - output_13_loss: 0.1562 - output_14_loss: 0.5050 - output_15_loss: 0.7463 - output_16_loss: 0.4546\n",
      "\n",
      " Validation Accuracy - Numerical: 0.42776283429633716 Categorical: 0.07658971648128109, Total: 0.1863313157984861\n",
      "3/16. valid_mask: [1. 1. 1. 0. 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 612us/step - loss: 12.7295 - output_1_loss: 0.0829 - output_2_loss: 1.0234 - output_3_loss: 0.0599 - output_4_loss: 1.5157 - output_5_loss: 0.8277 - output_6_loss: 0.4890 - output_7_loss: 0.0888 - output_8_loss: 1.4698 - output_9_loss: 4.1048 - output_10_loss: 0.0347 - output_11_loss: 0.6861 - output_12_loss: 0.5871 - output_13_loss: 0.1425 - output_14_loss: 0.4847 - output_15_loss: 0.6976 - output_16_loss: 0.4346\n",
      "\n",
      " Validation Accuracy - Numerical: 0.3338877702365896 Categorical: 0.07575934681293157, Total: 0.1564244791328247\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 606us/step - loss: 13.4022 - output_1_loss: 0.0558 - output_2_loss: 0.9707 - output_3_loss: 0.0617 - output_4_loss: 2.1372 - output_5_loss: 1.0270 - output_6_loss: 0.5164 - output_7_loss: 0.3820 - output_8_loss: 1.3878 - output_9_loss: 3.9717 - output_10_loss: 0.0287 - output_11_loss: 0.6489 - output_12_loss: 0.5377 - output_13_loss: 0.1365 - output_14_loss: 0.4732 - output_15_loss: 0.6607 - output_16_loss: 0.4059\n",
      "\n",
      " Validation Accuracy - Numerical: 0.48485149122965226 Categorical: 0.07124701403933968, Total: 0.20049841316131237\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 604us/step - loss: 11.2499 - output_1_loss: 0.0451 - output_2_loss: 0.8951 - output_3_loss: 0.0585 - output_4_loss: 1.2081 - output_5_loss: 0.7441 - output_6_loss: 0.3467 - output_7_loss: 0.0695 - output_8_loss: 1.3250 - output_9_loss: 3.8472 - output_10_loss: 0.0208 - output_11_loss: 0.6141 - output_12_loss: 0.5068 - output_13_loss: 0.1280 - output_14_loss: 0.4464 - output_15_loss: 0.6214 - output_16_loss: 0.3731\n",
      "\n",
      " Validation Accuracy - Numerical: 0.31584290022069766 Categorical: 0.07389294471128112, Total: 0.1495023058079738\n",
      "4/16. valid_mask: [1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 10s 599us/step - loss: 14.7783 - output_1_loss: 0.0544 - output_2_loss: 0.9150 - output_3_loss: 0.0516 - output_4_loss: 3.3824 - output_5_loss: 1.1287 - output_6_loss: 1.3273 - output_7_loss: 0.0662 - output_8_loss: 1.3092 - output_9_loss: 3.7433 - output_10_loss: 0.0225 - output_11_loss: 0.6267 - output_12_loss: 0.5186 - output_13_loss: 0.1398 - output_14_loss: 0.4588 - output_15_loss: 0.6367 - output_16_loss: 0.3969\n",
      "\n",
      " Validation Accuracy - Numerical: 0.3961612858901451 Categorical: 0.07212597330564663, Total: 0.1733870084883024\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 600us/step - loss: 13.5464 - output_1_loss: 0.0369 - output_2_loss: 0.8264 - output_3_loss: 0.0457 - output_4_loss: 2.9299 - output_5_loss: 0.9336 - output_6_loss: 1.2264 - output_7_loss: 0.0954 - output_8_loss: 1.2248 - output_9_loss: 3.6172 - output_10_loss: 0.0179 - output_11_loss: 0.5857 - output_12_loss: 0.4732 - output_13_loss: 0.1226 - output_14_loss: 0.4396 - output_15_loss: 0.5960 - output_16_loss: 0.3750\n",
      "\n",
      " Validation Accuracy - Numerical: 0.3635077757943154 Categorical: 0.07216387680326432, Total: 0.1632088452379678\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 601us/step - loss: 13.8372 - output_1_loss: 0.0415 - output_2_loss: 0.7901 - output_3_loss: 0.0475 - output_4_loss: 3.2024 - output_5_loss: 1.0336 - output_6_loss: 1.3702 - output_7_loss: 0.1051 - output_8_loss: 1.1901 - output_9_loss: 3.5127 - output_10_loss: 0.0188 - output_11_loss: 0.5718 - output_12_loss: 0.4656 - output_13_loss: 0.1176 - output_14_loss: 0.4308 - output_15_loss: 0.5727 - output_16_loss: 0.3669\n",
      "\n",
      " Validation Accuracy - Numerical: 0.42191821708710026 Categorical: 0.0741107139031463, Total: 0.1828005586481319\n",
      "5/16. valid_mask: [1. 1. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17526/17526 [==============================] - 10s 597us/step - loss: 11.2709 - output_1_loss: 0.0371 - output_2_loss: 0.7454 - output_3_loss: 0.0713 - output_4_loss: 1.8062 - output_5_loss: 0.8149 - output_6_loss: 0.5157 - output_7_loss: 0.3653 - output_8_loss: 1.1083 - output_9_loss: 3.3764 - output_10_loss: 0.0165 - output_11_loss: 0.5467 - output_12_loss: 0.4456 - output_13_loss: 0.1125 - output_14_loss: 0.4219 - output_15_loss: 0.5405 - output_16_loss: 0.3468\n",
      "\n",
      " Validation Accuracy - Numerical: 0.4517911281845056 Categorical: 0.07198659886553556, Total: 0.1906755142777137\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 10s 597us/step - loss: 11.1065 - output_1_loss: 0.0333 - output_2_loss: 0.6899 - output_3_loss: 0.0652 - output_4_loss: 1.9326 - output_5_loss: 0.9841 - output_6_loss: 0.4559 - output_7_loss: 0.3444 - output_8_loss: 1.0310 - output_9_loss: 3.2537 - output_10_loss: 0.0135 - output_11_loss: 0.5107 - output_12_loss: 0.4248 - output_13_loss: 0.1084 - output_14_loss: 0.4137 - output_15_loss: 0.5132 - output_16_loss: 0.3324\n",
      "\n",
      " Validation Accuracy - Numerical: 0.47086422529581534 Categorical: 0.07161687892406676, Total: 0.1963816746652382\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 602us/step - loss: 10.5733 - output_1_loss: 0.0308 - output_2_loss: 0.6604 - output_3_loss: 0.0594 - output_4_loss: 1.7978 - output_5_loss: 0.9305 - output_6_loss: 0.3799 - output_7_loss: 0.3503 - output_8_loss: 0.9652 - output_9_loss: 3.1367 - output_10_loss: 0.0149 - output_11_loss: 0.4978 - output_12_loss: 0.4116 - output_13_loss: 0.1031 - output_14_loss: 0.4041 - output_15_loss: 0.5039 - output_16_loss: 0.3269\n",
      "\n",
      " Validation Accuracy - Numerical: 0.43873054466284317 Categorical: 0.07254270375657769, Total: 0.18697640403978566\n",
      "6/16. valid_mask: [1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 10s 595us/step - loss: 12.0920 - output_1_loss: 0.0648 - output_2_loss: 0.6836 - output_3_loss: 0.0590 - output_4_loss: 1.0592 - output_5_loss: 0.5513 - output_6_loss: 0.4694 - output_7_loss: 0.3716 - output_8_loss: 1.0031 - output_9_loss: 3.0926 - output_10_loss: 0.2541 - output_11_loss: 1.6899 - output_12_loss: 1.1338 - output_13_loss: 0.1195 - output_14_loss: 0.4358 - output_15_loss: 0.5503 - output_16_loss: 0.5540\n",
      "\n",
      " Validation Accuracy - Numerical: 0.4278550017234835 Categorical: 0.07241884361712331, Total: 0.18349264302536086\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 10s 593us/step - loss: 9.7983 - output_1_loss: 0.0325 - output_2_loss: 0.5895 - output_3_loss: 0.0531 - output_4_loss: 0.5157 - output_5_loss: 0.4256 - output_6_loss: 0.4864 - output_7_loss: 0.1230 - output_8_loss: 0.7979 - output_9_loss: 2.8586 - output_10_loss: 0.1291 - output_11_loss: 1.4470 - output_12_loss: 0.9622 - output_13_loss: 0.0929 - output_14_loss: 0.3956 - output_15_loss: 0.4682 - output_16_loss: 0.4212\n",
      "\n",
      " Validation Accuracy - Numerical: 0.34492131084678324 Categorical: 0.07291411283999245, Total: 0.15791636221711458\n",
      "sub_epochs 3/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 10s 597us/step - loss: 9.2859 - output_1_loss: 0.0279 - output_2_loss: 0.5567 - output_3_loss: 0.0588 - output_4_loss: 0.4604 - output_5_loss: 0.3684 - output_6_loss: 0.4391 - output_7_loss: 0.1259 - output_8_loss: 0.7389 - output_9_loss: 2.7493 - output_10_loss: 0.1161 - output_11_loss: 1.4129 - output_12_loss: 0.9291 - output_13_loss: 0.0819 - output_14_loss: 0.3777 - output_15_loss: 0.4488 - output_16_loss: 0.3940\n",
      "\n",
      " Validation Accuracy - Numerical: 0.35088444566077726 Categorical: 0.0739628597025004, Total: 0.1605008553144619\n",
      "7/16. valid_mask: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1.] Start!\n",
      "\n",
      "sub_epochs 1/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "17526/17526 [==============================] - 11s 605us/step - loss: 11.1771 - output_1_loss: 0.0454 - output_2_loss: 0.6194 - output_3_loss: 0.0608 - output_4_loss: 0.5010 - output_5_loss: 0.4205 - output_6_loss: 0.3528 - output_7_loss: 0.1279 - output_8_loss: 0.7511 - output_9_loss: 2.7560 - output_10_loss: 0.0556 - output_11_loss: 0.6962 - output_12_loss: 0.6211 - output_13_loss: 0.8632 - output_14_loss: 1.3394 - output_15_loss: 1.6101 - output_16_loss: 0.3565\n",
      "\n",
      " Validation Accuracy - Numerical: 0.34090537829157086 Categorical: 0.07375711869584736, Total: 0.15724094981951095\n",
      "sub_epochs 2/3 Start!\n",
      "\n",
      "Epoch 1/1\n",
      "15872/17526 [==========================>...] - ETA: 0s - loss: 9.4833 - output_1_loss: 0.0208 - output_2_loss: 0.5287 - output_3_loss: 0.0555 - output_4_loss: 0.2622 - output_5_loss: 0.2295 - output_6_loss: 0.2618 - output_7_loss: 0.1170 - output_8_loss: 0.6661 - output_9_loss: 2.6149 - output_10_loss: 0.0225 - output_11_loss: 0.5806 - output_12_loss: 0.5132 - output_13_loss: 0.7243 - output_14_loss: 1.1746 - output_15_loss: 1.4247 - output_16_loss: 0.2869"
     ]
    }
   ],
   "source": [
    "## 원하는 epochs 입력\n",
    "epochs = 10\n",
    "sub_epochs = 3\n",
    "\n",
    "for i in range(epochs) :\n",
    "    print(\"epochs {}/{}  \\n\".format(i+1,epochs))\n",
    "        \n",
    "    for j, valid in enumerate(valid_list) :\n",
    "        print(\"{}/{}. valid_mask:\".format(j+1, len(valid_list)),valid,\"Start!\\n\")\n",
    "\n",
    "        for k in range(sub_epochs) :\n",
    "\n",
    "            ## validation set 만듦\n",
    "\n",
    "            sub_train_data, val_data = make_val(train_data, (len(train_data)*3)//10)\n",
    "\n",
    "            ##mask 씌우는 작업\n",
    "            masked_sub_train_data = sub_train_data * valid\n",
    "            masked_val_data = val_data * valid\n",
    "\n",
    "\n",
    "            ## 카테고리컬 함수만 one-hot encoding으로 변환\n",
    "\n",
    "            train_cate_1 = to_categorical(sub_train_data[:,0], num_classes=max(train_data[:,0]).astype(int)+1)\n",
    "            train_cate_2 = to_categorical(sub_train_data[:,1], num_classes=max(train_data[:,1]).astype(int)+1)\n",
    "            train_cate_3 = to_categorical(sub_train_data[:,7], num_classes=max(train_data[:,7]).astype(int)+1)\n",
    "            train_cate_4 = to_categorical(sub_train_data[:,8], num_classes=max(train_data[:,8]).astype(int)+1)\n",
    "            train_cate_5 = to_categorical(sub_train_data[:,9], num_classes=max(train_data[:,9]).astype(int)+1)\n",
    "            train_cate_6 = to_categorical(sub_train_data[:,10], num_classes=max(train_data[:,10]).astype(int)+1)\n",
    "            train_cate_7 = to_categorical(sub_train_data[:,11], num_classes=max(train_data[:,11]).astype(int)+1)\n",
    "            train_cate_8 = to_categorical(sub_train_data[:,12], num_classes=max(train_data[:,12]).astype(int)+1)\n",
    "            train_cate_9 = to_categorical(sub_train_data[:,13], num_classes=max(train_data[:,13]).astype(int)+1)\n",
    "            train_cate_10 = to_categorical(sub_train_data[:,14], num_classes=max(train_data[:,14]).astype(int)+1)\n",
    "            train_cate_11 = to_categorical(sub_train_data[:,15], num_classes=max(train_data[:,15]).astype(int)+1)\n",
    "\n",
    "            val_cate_1 = to_categorical(val_data[:,0], num_classes=max(train_data[:,0]).astype(int)+1)\n",
    "            val_cate_2 = to_categorical(val_data[:,1], num_classes=max(train_data[:,1]).astype(int)+1)\n",
    "            val_cate_3 = to_categorical(val_data[:,7], num_classes=max(train_data[:,7]).astype(int)+1)\n",
    "            val_cate_4 = to_categorical(val_data[:,8], num_classes=max(train_data[:,8]).astype(int)+1)\n",
    "            val_cate_5 = to_categorical(val_data[:,9], num_classes=max(train_data[:,9]).astype(int)+1)\n",
    "            val_cate_6 = to_categorical(val_data[:,10], num_classes=max(train_data[:,10]).astype(int)+1)\n",
    "            val_cate_7 = to_categorical(val_data[:,11], num_classes=max(train_data[:,11]).astype(int)+1)\n",
    "            val_cate_8 = to_categorical(val_data[:,12], num_classes=max(train_data[:,12]).astype(int)+1)\n",
    "            val_cate_9 = to_categorical(val_data[:,13], num_classes=max(train_data[:,13]).astype(int)+1)\n",
    "            val_cate_10 = to_categorical(val_data[:,14], num_classes=max(train_data[:,14]).astype(int)+1)\n",
    "            val_cate_11 = to_categorical(val_data[:,15], num_classes=max(train_data[:,15]).astype(int)+1)\n",
    "\n",
    "\n",
    "            print(\"sub_epochs {}/{} Start!\\n\".format(k+1,sub_epochs))\n",
    "\n",
    "            model.fit({'data_input' : masked_sub_train_data, 'valid_input' : np.array([valid]*len(masked_sub_train_data))},\n",
    "                     {'output_1': train_cate_1, \n",
    "                     'output_2': train_cate_2, \n",
    "                     'output_3': sub_train_data[:,2], \n",
    "                     'output_4': sub_train_data[:,3], \n",
    "                     'output_5': sub_train_data[:,4], \n",
    "                     'output_6': sub_train_data[:,5], \n",
    "                     'output_7': sub_train_data[:,6], \n",
    "                     'output_8': train_cate_3, \n",
    "                     'output_9': train_cate_4, \n",
    "                     'output_10': train_cate_5, \n",
    "                     'output_11': train_cate_6, \n",
    "                     'output_12': train_cate_7, \n",
    "                     'output_13': train_cate_8, \n",
    "                     'output_14': train_cate_9, \n",
    "                     'output_15': train_cate_10,\n",
    "                     'output_16': train_cate_11 },\n",
    "                      batch_size=32)\n",
    "\n",
    "\n",
    "            ## validation 평가\n",
    "\n",
    "            val_result = model.predict({'data_input' : masked_val_data, 'valid_input' : np.array([valid]*len(masked_val_data))})\n",
    "\n",
    "            numerical_score = 0\n",
    "            categorical_score = 0\n",
    "\n",
    "            ## 평가하기위해 결과 쪼갬 및 one hot decoding\n",
    "\n",
    "            result1 = np.array([[np.argmax(res)] for res in val_result[0]])\n",
    "            result2 = np.array([[np.argmax(res)] for res in val_result[1]])\n",
    "            result3 = val_result[2]\n",
    "            result4 = val_result[3]\n",
    "            result5 = val_result[4]\n",
    "            result6 = val_result[5]\n",
    "            result7 = val_result[6]\n",
    "            result8 = np.array([[np.argmax(res)] for res in val_result[7]])\n",
    "            result9 = np.array([[np.argmax(res)] for res in val_result[8]])\n",
    "            result10 = np.array([[np.argmax(res)] for res in val_result[9]])\n",
    "            result11 = np.array([[np.argmax(res)] for res in val_result[10]])\n",
    "            result12 = np.array([[np.argmax(res)] for res in val_result[11]])\n",
    "            result13 = np.array([[np.argmax(res)] for res in val_result[12]])\n",
    "            result14 = np.array([[np.argmax(res)] for res in val_result[13]])\n",
    "            result15 = np.array([[np.argmax(res)] for res in val_result[14]])\n",
    "            result16 = np.array([[np.argmax(res)] for res in val_result[15]])\n",
    "\n",
    "            ## numerical scoring\n",
    "\n",
    "            numerical_score += np.exp(0-np.mean(((result3 - val_data[:,2])/ 1)**2))\n",
    "            numerical_score += np.exp(0-np.mean(((result4 - val_data[:,3])/ 1)**2))\n",
    "            numerical_score += np.exp(0-np.mean(((result5 - val_data[:,4])/ 1)**2))\n",
    "            numerical_score += np.exp(0-np.mean(((result6 - val_data[:,5])/ 1)**2))\n",
    "            numerical_score += np.exp(0-np.mean(((result7 - val_data[:,6])/ 1)**2))\n",
    "\n",
    "\n",
    "            ## categorical scoring\n",
    "\n",
    "            categorical_score += np.mean((result1 == val_cate_1).astype(float))\n",
    "            categorical_score += np.mean((result2 == val_cate_2).astype(float))\n",
    "            categorical_score += np.mean((result8 == val_cate_3).astype(float))\n",
    "            categorical_score += np.mean((result9 == val_cate_4).astype(float))\n",
    "            categorical_score += np.mean((result10 == val_cate_5).astype(float))\n",
    "            categorical_score += np.mean((result11 == val_cate_6).astype(float))\n",
    "            categorical_score += np.mean((result12 == val_cate_7).astype(float))\n",
    "            categorical_score += np.mean((result13 == val_cate_8).astype(float))\n",
    "            categorical_score += np.mean((result14 == val_cate_9).astype(float))\n",
    "            categorical_score += np.mean((result15 == val_cate_10).astype(float))\n",
    "            categorical_score += np.mean((result16 == val_cate_11).astype(float))\n",
    "\n",
    "\n",
    "            print('\\n Validation Accuracy - Numerical: {} Categorical: {}, Total: {}\\n'\n",
    "                  .format(numerical_score/5, categorical_score/11, (numerical_score + categorical_score)/16))\n",
    "        \n",
    "    \n",
    "print('Finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict({'data_input' : test_data, 'valid_input' : np.array(test_masks)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## +[1] 은 label decoder를 위해 해줌\n",
    "\n",
    "result1 = np.array([[np.argmax(res)] for res in result[0]]) + [1]\n",
    "result2 = np.array([[np.argmax(res)] for res in result[1]]) + [1]\n",
    "result3 = result[2]\n",
    "result4 = result[3]\n",
    "result5 = result[4]\n",
    "result6 = result[5]\n",
    "result7 = result[6]\n",
    "result8 = np.array([[np.argmax(res)] for res in result[7]]) + [1]\n",
    "result9 = np.array([[np.argmax(res)] for res in result[8]]) + [1]\n",
    "result10 = np.array([[np.argmax(res)] for res in result[9]]) + [1]\n",
    "result11 = np.array([[np.argmax(res)] for res in result[10]]) + [1]\n",
    "result12 = np.array([[np.argmax(res)] for res in result[11]]) + [1]\n",
    "result13 = np.array([[np.argmax(res)] for res in result[12]]) + [1]\n",
    "result14 = np.array([[np.argmax(res)] for res in result[13]]) + [1]\n",
    "result15 = np.array([[np.argmax(res)] for res in result[14]]) + [1]\n",
    "result16 = np.array([[np.argmax(res)] for res in result[15]]) + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## column들 다시 합쳐주기\n",
    "\n",
    "result = np.hstack((result1, result2, result3, result4, result5, result6, result7, result8,\n",
    "         result9, result10, result11, result12, result13, result14, result15, result16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "## 역으로 되돌려 주기 위함\n",
    "merge = pd.read_csv(\"./dataset/raw_merge.csv\", encoding='CP949')\n",
    "\n",
    "decoded_result = []\n",
    "for i, col in enumerate(columns):\n",
    "    if type(merge[col][0]) == str:\n",
    "        le=LabelEncoder()\n",
    "        y=merge[col].tolist()\n",
    "        le.fit(y)\n",
    "        z = le.inverse_transform(result[:,i].astype(int))\n",
    "        z = z.reshape(-1,1)\n",
    "        ## decoded_result 에 아무것도 없으면 hstack 하는데 에러남\n",
    "        if not len(decoded_result):\n",
    "            decoded_result = z\n",
    "        else:\n",
    "            decoded_result = np.hstack((decoded_result,z))\n",
    "        \n",
    "    else:\n",
    "        z = result[:,i]\n",
    "        z = z.reshape(-1,1)\n",
    "        decoded_result = np.hstack((decoded_result,z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoded_result file을 data frame으로 바꿔서 csv파일로 저장\n",
    "\n",
    "decoded_result_df = {columns[0] : decoded_result[:,0], \n",
    "                     columns[1] : decoded_result[:,1], \n",
    "                     columns[2] : decoded_result[:,2], \n",
    "                     columns[3] : decoded_result[:,3], \n",
    "                     columns[4] : decoded_result[:,4], \n",
    "                     columns[5] : decoded_result[:,5], \n",
    "                     columns[6] : decoded_result[:,6], \n",
    "                     columns[7] : decoded_result[:,7], \n",
    "                     columns[8] : decoded_result[:,8], \n",
    "                     columns[9] : decoded_result[:,9], \n",
    "                     columns[10] : decoded_result[:,10], \n",
    "                     columns[11] : decoded_result[:,11], \n",
    "                     columns[12] : decoded_result[:,12], \n",
    "                     columns[13] : decoded_result[:,13], \n",
    "                     columns[14] : decoded_result[:,14], \n",
    "                     columns[15] : decoded_result[:,15], }\n",
    "\n",
    "decoded_result_df = pd.DataFrame(decoded_result_df, columns = columns)\n",
    "\n",
    "decoded_result_df.to_csv(\"./dataset/decoded_result.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_kor = pd.read_csv(\"./result_kor.csv\",encoding = 'CP949')\n",
    "result_kor_HH = pd.read_csv(\"./result_kor.csv\",encoding = 'CP949')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [\"행\",\"열\"]:\n",
    "    if type(result_kor[col][0]) == str:\n",
    "        # 열 데이터 처리\n",
    "        le=LabelEncoder()\n",
    "        y=result_kor[col].tolist()\n",
    "        le.fit(y)\n",
    "        result_kor[col]=le.transform(y)\n",
    "    else:\n",
    "        # 행 데이터 처리\n",
    "        y = result_kor[col].tolist()\n",
    "        for i in range(len(y)):\n",
    "            #list 위치에 대한 보정값\n",
    "            y[i] -= 2\n",
    "            \n",
    "        result_kor[col] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:194: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "for i in range(result_kor.shape[0]):\n",
    "    result_kor_HH[\"값\"][i] = decoded_result[result_kor[\"행\"][i],result_kor[\"열\"][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_kor_HH.to_csv(\"./result_kor_HH.csv\", encoding=\"CP949\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
