{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import functools\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../NYC/train.csv')\n",
    "test = pd.read_csv('../NYC/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 깨달음\n",
    " - merge 함부로 하는 거 아니다, 중복을 항상 살펴라\n",
    " - apply 쓰지말고, np 연산을 쓰자\n",
    " - clustering MiniBatchKMeans, Birch, KMeans 다양한 알고리즘이 있다.\n",
    " \n",
    "## 반성\n",
    " - API 다른 사람이 올려줬는데, 그냥 그대로 쓰지말고 나도 API 가져다 쓸 수 있게 하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train, test = addFeature(train.copy(), test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1450352, 26) (625134, 24)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_set2.csv', index = False)\n",
    "test.to_csv('test_set2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addFeature(train, test):\n",
    "    #train, test Data\n",
    "    \n",
    "    #Time Data\n",
    "    for df in [train, test]:\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "        #df['month'] = df['pickup_datetime'].dt.month\n",
    "        df['yday'] = df['pickup_datetime'].dt.dayofyear\n",
    "        df['wday'] = df['pickup_datetime'].dt.dayofweek\n",
    "        df['hour'] = df['pickup_datetime'].dt.hour\n",
    "        #df['whour'] = 0\n",
    "        #df['whour'].loc[(df['hour']<17) & (df['hour']>9)] = 1\n",
    "        df['store_and_fwd_flag'] = df['store_and_fwd_flag'].map({'N':0, 'Y':1})\n",
    "    \n",
    "    train, test = clusterFeature(train, test)\n",
    "    train, test = distanceFeature(train, test)\n",
    "    train = cleanDf(train)\n",
    "    train, test = mergeRouteWeather(train, test)\n",
    "    #train, test =dummyBuilding(train,test) #Regard of Weather and Route, the potential variables exsits\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "def clusterFeature(train, test):\n",
    "    coord = np.vstack([train[['pickup_longitude', 'pickup_latitude']].values, train[['dropoff_longitude', 'dropoff_latitude']].values])\n",
    "    sample_ind = np.random.permutation(len(coord))[:500000]\n",
    "    kmeans = MiniBatchKMeans(n_clusters = 100, batch_size = 10000).fit(coord[sample_ind])\n",
    "    train.loc[:,'pickup_cluster'] = kmeans.predict(train[['pickup_longitude', 'pickup_latitude']])\n",
    "    train.loc[:,'dropoff_cluster'] = kmeans.predict(train[['dropoff_longitude', 'dropoff_latitude']])\n",
    "    test.loc[:,'pickup_cluster'] = kmeans.predict(test[['pickup_longitude', 'pickup_latitude']])\n",
    "    test.loc[:,'dropoff_cluster'] = kmeans.predict(test[['dropoff_longitude', 'dropoff_latitude']])\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Direct and Manhatten Distance\n",
    "https://www.kaggle.com/headsortails/nyc-taxi-eda-update-the-fast-the-curious\n",
    "2. Distacne JFK/Laguarid_jfk\n",
    "https://www.kaggle.io/svf/1512124/63b262954485433518d65fb2927eb518/__results__.html#direct-distance-of-the-trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def distanceFeature(train, test):\n",
    "    jfk_coord  = [40.639722, -73.778889]\n",
    "    la_guardia_coord = [40.77725, -73.872611]\n",
    "    for df in [train, test]:\n",
    "        df.loc[:, 'jfk_lat'] = jfk_coord[0]\n",
    "        df.loc[:, 'jfk_lon'] = jfk_coord[1]\n",
    "        df.loc[:, 'la_lat'] = la_guardia_coord[0]\n",
    "        df.loc[:, 'la_lon'] = la_guardia_coord[1]\n",
    "    #Direct Distance\n",
    "        df.loc[:, 'distance_haversine'] = haversine_array(df['pickup_latitude'].values, df['pickup_longitude'].values, df['dropoff_latitude'].values, df['dropoff_longitude'].values) \n",
    "    #Mahattan Distance\n",
    "        df.loc[:, 'distance_dummy_manhattan'] =  dummy_manhattan_distance(df['pickup_latitude'].values, df['pickup_longitude'].values, df['dropoff_latitude'].values, df['dropoff_longitude'].values)\n",
    "    #JFK Distance     \n",
    "        df.loc[:,'distance_jfk_pick'] = haversine_array(df['pickup_latitude'].values, df['pickup_longitude'].values, df['jfk_lat'].values, df['jfk_lon'].values)\n",
    "        df.loc[:,'distance_jfk_drop'] = haversine_array(df['dropoff_latitude'].values, df['dropoff_longitude'].values, df['jfk_lat'].values, df['jfk_lon'].values)\n",
    "    #Airport Distance    \n",
    "        df.loc[:,'distance_laguardia_pick'] = haversine_array(df['pickup_latitude'].values, df['pickup_longitude'].values, df['la_lat'].values, df['la_lon'].values)\n",
    "        df.loc[:,'distance_laguardia_drop'] = haversine_array(df['dropoff_latitude'].values, df['dropoff_longitude'].values, df['la_lat'].values, df['la_lon'].values)\n",
    "        \n",
    "        df.drop(['jfk_lat', 'jfk_lon', 'la_lat', 'la_lon'], axis = 1, inplace = True)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Outlier\n",
    "https://www.kaggle.io/svf/1512124/63b262954485433518d65fb2927eb518/__results__.html#direct-distance-of-the-trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanDf(train):\n",
    "    train = train.loc[(train['trip_duration'] < 22*3600)]\n",
    "    train = train.loc[(train['distance_haversine'] > 0) | ((train['distance_haversine'] == 0) & (train['trip_duration'] < 60))]\n",
    "    train = train.loc[(train['distance_jfk_pick'] < 3e5 ) & (train['distance_jfk_drop']<3e5)]\n",
    "    train = train.loc[(train['trip_duration'] > 10)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mergeRouteWeather(train, test):\n",
    "    \n",
    "    #Fast Data\n",
    "    fast1 = pd.read_csv('../NYC/AdditionalSource/fastest_routes_train_part_1.csv', usecols = ['id','total_distance', 'total_travel_time', 'number_of_steps'])\n",
    "    fast2 = pd.read_csv('../NYC/AdditionalSource/fastest_routes_train_part_2.csv', usecols = ['id','total_distance', 'total_travel_time', 'number_of_steps'])\n",
    "    fast = pd.concat([fast1, fast2], axis = 0)\n",
    "    #Weather Data\n",
    "    weather = pd.read_csv('../NYC/AdditionalSource/KNYC.csv', usecols = ['Time','Conditions'])\n",
    "    weather['Time'] = pd.to_datetime(weather['Time'])\n",
    "    weather['yday'] = weather['Time'].dt.dayofyear\n",
    "    weather['hour'] = weather['Time'].dt.hour\n",
    "    weather.drop(['Time'], axis =1, inplace = True)\n",
    "    weather.rename(columns = {'Conditions' : 'condition'}, inplace = True)\n",
    "    weather.drop_duplicates(subset = ['yday', 'hour'], keep = 'first', inplace = True)\n",
    "    \n",
    "    #Merging Fast And Weather\n",
    "    ix = 0\n",
    "    for df in [train, test]:\n",
    "        df = pd.merge(df, fast, on = ['id'], how = 'left')\n",
    "        df = pd.merge(df, weather, on = ['yday', 'hour'], how = 'left')\n",
    "        if ix == 0: train = df\n",
    "        else: test = df\n",
    "        ix += 1\n",
    "    \n",
    "    return train, test\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get dummies by Pandas, adding prefix and seperation\n",
    "https://www.kaggle.com/karelrv/nyct-from-a-to-z-with-xgboost-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummyBuilding(train, test):\n",
    "    ix = 0\n",
    "    for df in [train, test]:\n",
    "        df_pickup_cluster = pd.get_dummies(df['pickup_cluster'], prefix = 'p', prefix_sep = '_')\n",
    "        df_dropoff_cluster = pd.get_dummies(df['dropoff_cluster'], prefix = 'd', prefix_sep = '_')\n",
    "        #df_month = pd.get_dummies(df['month'], prefix = 'm', prefix_sep = '_')\n",
    "        df_wday = pd.get_dummies(df['wday'], prefix = 'd', prefix_sep ='_')\n",
    "        df_hour = pd.get_dummies(df['hour'], prefix = 'h', prefix_sep = '_')\n",
    "        #df_condition = pd.get_dummies(df['condition'], prefix = 'c', prefix_sep = '_')\n",
    "        \n",
    "        #if ix == 1: df_passenger_count = df_passenger_count.drop('pc_9', axis = 1)\n",
    "        #check condition regard of test and train\n",
    "        \n",
    "        df.drop(['passenger_count','pickup_datetime'], axis = 1, inplace = True)\n",
    "        df.drop(['wday', 'hour', 'condition'], axis = 1, inplace = True)\n",
    "        if ix == 0: df.drop(['pickup_longitude','pickup_latitude','dropoff_longitude','dropoff_latitude'], axis =1, inplace = True)\n",
    "        else: df.drop(['pickup_longitude','pickup_latitude'], axis =1, inplace = True)\n",
    "        \n",
    "        df_Master = pd.concat([df, df_pickup_cluster, df_dropoff_cluster, \n",
    "                              df_wday, df_hour, df_condition], axis = 1)\n",
    "        del df, df_pickup_cluster, df_dropoff_cluster, df_wday, df_hour, df_condition\n",
    "        \n",
    "        if ix == 0: train = df_Master\n",
    "        else: test = df_Master\n",
    "        ix += 1\n",
    "    \n",
    "    return train, test    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
