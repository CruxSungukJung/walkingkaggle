---
title: "02.keras"
author: "syleeie"
date: '2018 3 10 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(keras)
library(tidyverse)
library(qdapRegex)
library(data.table)
```

```{r}
train_data = read_csv("./data/train.csv")
test_data = read_csv("./data/test.csv")

max_words = 30000
maxl = 100
```

```{r}
wordseq = text_tokenizer(num_words = max_words) %>%
        fit_text_tokenizer(c(train_data$comment_text,test_data$comment_text))
```

```{r}
print(wordseq,10)
```

```{r}
#word dictionary
word_index = wordseq$word_index

x_train = texts_to_sequences(wordseq, train_data$comment_text ) %>%
        pad_sequences( maxlen = maxl)
y_train = as.matrix(train_data[,3:8])

x_test = texts_to_sequences(wordseq, test_data$comment_text ) %>%
        pad_sequences( maxlen = maxl)

cat("beginning the word embedding \n")
```


```{r}
wgt = fread("./data/glove.840B.300d.txt", data.table = FALSE)  %>%
        rename(word=V1)  %>%
        mutate(word=gsub("[[:punct:]]"," ", rm_white(word) ))

dic_words = wgt$word
wordindex = unlist(wordseq$word_index)
```


```{r}
dic = data.frame(word=names(wordindex), key = wordindex,row.names = NULL) %>%
        arrange(key) %>% 
        .[1:max_words,]

w_embed = dic %>% 
        left_join(wgt)
```


```{r}
J = ncol(w_embed)
ndim = J-2

w_embed = w_embed [1:(max_words-1),3:J] %>%
        mutate_all(as.numeric) %>%
        mutate_all(round,6) %>%
        #fill na with 0
        mutate_all(funs(replace(., is.na(.), 0))) 

colnames(w_embed) = paste0("V",1:ndim)
w_embed = rbind(rep(0, ndim), w_embed) %>%
        as.matrix()

#good weight format for the layer_embedding
w_embed = list(array(w_embed , c(max_words, ndim)))

cat("beginning the neural network \n")
```


```{r}
inp = layer_input(shape = list(maxl),
                  dtype = "int32", name = "input")

model = inp %>%
        layer_embedding(input_dim = max_words, output_dim = ndim, input_length = maxl, weights = w_embed, trainable=FALSE) %>%
        layer_spatial_dropout_1d(rate=0.2) %>%
        bidirectional(
                layer_gru(units = 80, return_sequences = TRUE) 
        )

max_pool = model %>% layer_global_max_pooling_1d()
ave_pool = model %>% layer_global_average_pooling_1d()

outp = layer_concatenate(list(ave_pool, max_pool)) %>%
        layer_dense(units = 6, activation = "sigmoid")

model = keras_model(inp, outp)

model %>% compile(
        optimizer = "adam",
        loss = "binary_crossentropy",
        metrics = c("acc")
)

model
```


```{r}
history = model %>% fit(
        x_train, y_train,
        epochs = 20,
        batch_size = 32,
        validation_split = 0.05,
        callbacks = list(
                callback_model_checkpoint(paste0("toxic_comment_model.h5"), save_best_only = TRUE),
                callback_early_stopping(monitor = "val_loss", min_delta = 0, patience = 0,
                                        verbose = 0, mode = c("auto", "min", "max"))
        )
)
```



```{r}
model = load_model_hdf5(paste0("toxic_comment_model.h5"))

cat("beginning the prediction & submission \n")
```



```{r}
pred = model %>%
        predict(x_test, batch_size = 1024) %>%
        as.data.frame()

pred = cbind(id=test_data$id, pred) 

names(pred)[2:7] = c("toxic", "severe_toxic", "obscene", "threat","insult", "identity_hate")

write_csv(pred,"submission.csv")

```